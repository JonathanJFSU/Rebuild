docker run -v ~/docker_output:/app/outputs --gpus all --rm -it linux_amd64:latest 

//////////DOCKER COMMANDS////////////////


///BUILD COMMANDS///

///build on multiple OS and architecture
docker buildx create --use
docker buildx build --no-cache --platform linux/amd64 -t jonathanj19/siglip:linuxAMD-20.04 --push .       ###linux AMD    
docker buildx build --no-cache --platform linux/arm64 -t jonathanj19/siglip:linuxARM-20.04 --push .         ###linux ARM
**********************************************my reposioty/name:tag

///Build on machine only, do not push
/////for tesing you need --load so it is saved locally
docker buildx build --platform linux/amd64 -t wednite --load .

///remove caches, complete new build
--no-cache 


///RUN COMMANDS///

///CURRENT RUN COMMAND NOTE: Different for ARM and AMD as they require different commands to use GPUs
docker run -v ~/docker_output:/app/outputs --gpus all --rm -it linux_amd64:latest 
sudo docker run --rm -it --network host --runtime=nvidia jonathanj19/siglip:linuxARM-20.04


///run
docker run --rm -it  <imagename>        --rm no changes to container once closed, -it interactive

///run on gpu
--gpus all

///add volume
-v ~/docker_output:/app/output          ~/docker_ouptut is location on host machine, /app/oupout is location in container where vol is mounted


///PUSH COMMANDS///
///pushing built. 1. tag proeprly, if not, 2 push it, push it real good
1. docker tag <local-image-name> <dockerhub-username>/<repository-name>:<tag>
2. docker push <dockerhub-username>/<repository-name>:<tag>


///RUN PROGRAM IN CONTAINER///
python3 program.py --video "testVideo.mp4" --output_txt "result.txt" --    output_video "result.mp4"


/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
///PYTHON COMMANDS///

///check if cuda is available
python -c "import torch; print(torch.cuda.is_available()); print(torch.version. cuda)"


////check_gpu_torch.py
import torch

if torch.cuda.is_available():
    print("✅ PyTorch GPU is available:")
    print(f"  - Device name: {torch.cuda.get_device_name(0)}")
    print(f"  - Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
else:
    print("❌ PyTorch did NOT detect a GPU.")


/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
///BENCHMARK AND TESTING///
 //benchmark program (not on fedora)
 sudo apt install -y glmark2
 glmark2

 ///see if gpu is active ARM
 grastats
 GR3D_FREQ n% shows GPU is actively working


///see if gpu is active AMD
nvidia-smi --loop=1